{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from streamlit_jupyter import StreamlitPatcher\n",
    "StreamlitPatcher().jupyter()  # register streamlit with jupyter-compatible wrappers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.SQLagent import build_sql_agent\n",
    "from agents.csv_chat import build_csv_agent\n",
    "from utils.utility import ExcelLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import (SystemMessage, HumanMessage, AIMessage)\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Qdrant\n",
    "import streamlit as st\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 16:38:10.821 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "st.session_state.csv_file_paths = []\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Use the following pieces of context enclosed by triple backquotes to answer the question at the end.\n",
    "\\n\\n\n",
    "Context:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\\n\\n\n",
    "Question: [][][][]{question}[][][][]\n",
    "\\n\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_ai_key():\n",
    "    with st.sidebar:\n",
    "        openai_api_key = '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'\n",
    "        \"[Get an OpenAI API key](https://platform.openai.com/account/api-keys)\"\n",
    "        if not openai_api_key:\n",
    "            st.info(\"Please add your OpenAI API key to continue.\")\n",
    "            st.stop()\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    return openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_data\n",
    "def dbActive():\n",
    "    os.environ['DB_ACTIVE'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_page() -> None:\n",
    "    st.set_page_config(\n",
    "    )\n",
    "    st.sidebar.title(\"Options\")\n",
    "    icon, title = st.columns([3, 20])\n",
    "    with icon:\n",
    "        st.image('./img/image.png')\n",
    "    with title:\n",
    "        st.title('Finance Chatbot')\n",
    "    st.session_state['db_active'] = False\n",
    "def init_messages() -> None:\n",
    "    clear_button = st.sidebar.button(\"Clear Conversation\", key=\"clear\")\n",
    "    if clear_button or \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"You are a helpful AI QA assistant. \"\n",
    "                    \"When answering questions, use the context provided to you.\"\n",
    "                    \"If you don't know the answer, just say that you don't know, \"\n",
    "                    \"don't try to make up an answer. \"\n",
    "                    )\n",
    "            )\n",
    "        ]\n",
    "        st.session_state.costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_file(file_path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Function to load PDF text and split it into chunks.\n",
    "    \"\"\"\n",
    "    # Replace st.header and st.file_uploader with the file_path parameter\n",
    "\n",
    "    # Example:\n",
    "    # st.header(\"Upload Document or Connect to a Databse\")\n",
    "    # file_path = \"/path/to/your/file.pdf\"\n",
    "\n",
    "    all_docs = []\n",
    "    csv_paths = []\n",
    "    all_files = []\n",
    "\n",
    "    Loader = None\n",
    "\n",
    "    if file_path.endswith(\".txt\"):\n",
    "        Loader = TextLoader\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        Loader = PyPDFLoader\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        Loader = Docx2txtLoader\n",
    "    elif file_path.endswith(\".csv\"):\n",
    "        # Assuming that the hardcoded path is for a CSV file\n",
    "        csv_paths.append(file_path)\n",
    "    elif file_path.endswith(\".xlsx\"):\n",
    "        # Assuming that the hardcoded path is for an Excel file\n",
    "        loader = ExcelLoader(file_path)\n",
    "        paths = loader.load()\n",
    "        csv_paths.extend(paths)\n",
    "    else:\n",
    "        raise ValueError('File type is not supported')\n",
    "\n",
    "    if Loader:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tpfile:\n",
    "            tpfile.write(file_path)\n",
    "            loader = Loader(tpfile.name)\n",
    "            docs = loader.load()\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "    if all_docs:\n",
    "        documents = text_splitter.split_documents(all_docs)\n",
    "        all_files.append(('docs', documents))\n",
    "    if csv_paths:\n",
    "        all_files.append(('csv', csv_paths))\n",
    "    all_files = tuple(all_files)\n",
    "\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_credentials(model_name, temperature, chain_mode='Database'):\n",
    "    \"\"\"\n",
    "    creates a form for the user to input database login credentials\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the form has already been submitted\n",
    "    \n",
    "    db_active = os.environ['DB_ACTIVE']\n",
    "    if db_active == \"true\":\n",
    "        print(db_active)\n",
    "\n",
    "        return st.session_state['models']\n",
    "        \n",
    "    else:\n",
    "        username = None\n",
    "        host = None\n",
    "        port = None\n",
    "        db = None\n",
    "        password = None\n",
    "        import time\n",
    "        pholder = st.empty()\n",
    "        \n",
    "        with pholder.form('Database_Login'):\n",
    "            st.write(\"Enter Database Credentials \")\n",
    "            username = st.text_input('Username').strip()\n",
    "            password = st.text_input('Password', type='password',).strip()\n",
    "            rdbs = st.selectbox('Select RDBS:',\n",
    "                                (\"Postgres\",\n",
    "                                'MS SQL Server/Azure SQL',\n",
    "                                \"MySQL\",\n",
    "                                \"Oracle\")\n",
    "                            )\n",
    "            port = st.number_input('Port')\n",
    "            host = st.text_input('Hostname').strip()\n",
    "            db = st.text_input('Database name').strip()\n",
    "\n",
    "            submitted = st.form_submit_button('Submit')\n",
    "\n",
    "        if submitted:\n",
    "            with st.spinner(\"Logging into database...\"):\n",
    "                \n",
    "                llm_chain, llm = init_agent(model_name=model_name,\n",
    "                                    temperature=temperature,\n",
    "                                    rdbs = rdbs,\n",
    "                                    username=username,\n",
    "                                    password=password,\n",
    "                                    port=port,\n",
    "                                    host=host,\n",
    "                                    database=db,\n",
    "                                    chain_mode = chain_mode)\n",
    "            st.session_state['models'] = (llm_chain, llm)\n",
    "            st.success(\"Login Success\")\n",
    "            os.environ['DB_ACTIVE'] = \"true\"\n",
    "            db_active = os.environ['DB_ACTIVE']\n",
    "            st.session_state['db_active'] = True\n",
    "            time.sleep(2)\n",
    "            pholder.empty()\n",
    "\n",
    "            # If the form has already been submitted, return the stored models\n",
    "        if db_active == \"true\":\n",
    "            #return st.session_state['models']\n",
    "            mds =  st.session_state['models']\n",
    "            st.write(\"Reached\")\n",
    "            return mds\n",
    "        else:\n",
    "            st.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vector_store(\n",
    "    docs: str, embeddings: Union[OpenAIEmbeddings, LlamaCppEmbeddings]) \\\n",
    "        -> Optional[Qdrant]:\n",
    "    \"\"\"\n",
    "    Store the embedding vectors of text chunks into vector store (Qdrant).\n",
    "    \"\"\"\n",
    "    \n",
    "    if docs:\n",
    "        with st.spinner(\"Loading FIle ...\"):\n",
    "            chroma = Chroma.from_documents(\n",
    "             docs, embeddings\n",
    "            )\n",
    "    \n",
    "        st.success(\"File Loaded Successfully!!\")\n",
    "    else:\n",
    "        chroma = None\n",
    "    return chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model \n",
    "\n",
    "def select_llm() -> Union[ChatOpenAI, LlamaCpp]:\n",
    "    \"\"\"\n",
    "    Read user selection of parameters in Streamlit sidebar.\n",
    "    \"\"\"\n",
    "    model_name = \"gpt-4\"\n",
    "    temperature = \"0.5\"\n",
    "    chain_mode = \"CSV|Excel\"\n",
    "    #api_key  = st.sidebar.text_input('OPENAI API Key')\n",
    "    \n",
    "    return model_name, temperature, chain_mode,# api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_agent(model_name: str, temperature: float, **kwargs) -> Union[ChatOpenAI, LlamaCpp]:\n",
    "    \"\"\"\n",
    "    Load LLM.\n",
    "    \"\"\"\n",
    "    llm_agent = None  # Initialize llm_agent with a default value\n",
    "    \n",
    "    if model_name.startswith(\"gpt-\"):\n",
    "        llm =  ChatOpenAI(temperature=temperature, model_name=model_name)\n",
    "    \n",
    "    elif model_name.startswith(\"text-dav\"):\n",
    "        llm =  OpenAI(temperature=temperature, model_name=model_name)\n",
    "    \n",
    "    elif model_name.startswith(\"llama-2-\"):\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        llm = LlamaCpp(\n",
    "            model_path=f\"./models/{model_name}.bin\",\n",
    "            input={\"temperature\": temperature,\n",
    "                   \"max_length\": 2048,\n",
    "                   \"top_p\": 1\n",
    "                   },\n",
    "            n_ctx=2048,\n",
    "            callback_manager=callback_manager,\n",
    "            verbose=False,  # True\n",
    "        )\n",
    "    chain_mode = kwargs['chain_mode']\n",
    "    if chain_mode == 'Database':\n",
    "        rdbs = kwargs['rdbs']\n",
    "        username = kwargs['username']\n",
    "        password = kwargs['password']\n",
    "        host = kwargs['host']\n",
    "        port = kwargs['port']\n",
    "        database = kwargs['database']\n",
    "        #print('----------------------------------------------------------------')\n",
    "        #st.write(print(rdbs,username,password,host,port,database ))\n",
    "        #print(rdbs,username,password,host,port,database )\n",
    "        llm_agent = build_sql_agent(llm=llm, rdbs=rdbs, username=username, password=password,\n",
    "                                    host=host, port=port, database=database)\n",
    "    if chain_mode == 'CSV|Excel':\n",
    "        file_paths = kwargs['csv']\n",
    "        if file_paths is not None:\n",
    "            with st.spinner(\"Loading CSV FIle ...\"):\n",
    "                llm_agent = build_csv_agent(llm, file_path=file_paths)\n",
    "    \n",
    "    return llm_agent, llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieval_chain(model_name: str, temperature: float, **kwargs) -> Union[ChatOpenAI, LlamaCpp]:\n",
    "    if model_name.startswith(\"gpt-\"):\n",
    "        llm =  ChatOpenAI(temperature=temperature, model_name=model_name)\n",
    "    \n",
    "    elif model_name.startswith(\"text-dav\"):\n",
    "        llm =  OpenAI(temperature=temperature, model_name=model_name)\n",
    "    \n",
    "    elif model_name.startswith(\"llama-2-\"):\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        llm = LlamaCpp(\n",
    "            model_path=f\"./models/{model_name}.bin\",\n",
    "            input={\"temperature\": temperature,\n",
    "                   \"max_length\": 2048,\n",
    "                   \"top_p\": 1\n",
    "                   },\n",
    "            n_ctx=2048,\n",
    "            callback_manager=callback_manager,\n",
    "            verbose=False,  # True\n",
    "        )\n",
    "    docsearch = kwargs['docsearch']\n",
    "    retrieval_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "            llm,\n",
    "            retriever = docsearch.as_retriever(max_tokens_limit=4097)\n",
    "            )\n",
    "        \n",
    "    return retrieval_chain, llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model_name: str) -> Union[OpenAIEmbeddings, LlamaCppEmbeddings]:\n",
    "    \"\"\"\n",
    "    Load embedding model.\n",
    "    \"\"\"\n",
    "    if model_name.startswith(\"gpt-\") or model_name.startswith(\"text-dav\"):\n",
    "        return OpenAIEmbeddings()\n",
    "    elif model_name.startswith(\"llama-2-\"):\n",
    "        return LlamaCppEmbeddings(model_path=f\"./models/{model_name}.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(llm_chain,llm, message) -> tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Get the AI answer to user questions.\n",
    "    \"\"\"\n",
    "    import langchain\n",
    "\n",
    "    if isinstance(llm, (ChatOpenAI, OpenAI)):\n",
    "        with get_openai_callback() as cb:\n",
    "            try:\n",
    "                if isinstance(llm_chain, RetrievalQAWithSourcesChain):\n",
    "                    response = llm_chain(message)\n",
    "                    answer =  str(response['answer'])# + \"\\n\\nSOURCES: \" + str(response['sources'])\n",
    "                else:\n",
    "                    answer = llm_chain.run(message)\n",
    "            except langchain.schema.output_parser.OutputParserException as e:\n",
    "                response = str(e)\n",
    "                if not response.startswith(\"Could not parse tool input: \"):\n",
    "                    raise e\n",
    "                answer = response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
    "        return answer, cb.total_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_role(message: Union[SystemMessage, HumanMessage, AIMessage]) -> str:\n",
    "    \"\"\"\n",
    "    Identify role name from langchain.schema object.\n",
    "    \"\"\"\n",
    "    if isinstance(message, SystemMessage):\n",
    "        return \"system\"\n",
    "    if isinstance(message, HumanMessage):\n",
    "        return \"user\"\n",
    "    if isinstance(message, AIMessage):\n",
    "        return \"assistant\"\n",
    "    raise TypeError(\"Unknown message type.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_langchainschema_to_dict(\n",
    "        messages: List[Union[SystemMessage, HumanMessage, AIMessage]]) \\\n",
    "        -> List[dict]:\n",
    "    \"\"\"\n",
    "    Convert the chain of chat messages in list of langchain.schema format to\n",
    "    list of dictionary format.\n",
    "    \"\"\"\n",
    "    return [{\"role\": find_role(message),\n",
    "             \"content\": message.content\n",
    "             } for message in messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_userquesion_part_only(content):\n",
    "    \"\"\"\n",
    "    Function to extract only the user question part from the entire question\n",
    "    content combining user question and pdf context.\n",
    "    \"\"\"\n",
    "    content_split = content.split(\"[][][][]\")\n",
    "    if len(content_split) == 3:\n",
    "        return content_split[1]\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    import openai\n",
    "    init_page()\n",
    "    dbActive()\n",
    "    try:\n",
    "        key = open_ai_key()\n",
    "        if 'history' not in st.session_state:\n",
    "            st.session_state['history'] = []\n",
    "        \n",
    "        st.write(\"OpenAPI Key = \" + key )\n",
    "        model_name, temperature, chain_mode = select_llm()\n",
    "        st.write(\"Model = \" + model_name)\n",
    "        st.write(\"Temperature = \" + temperature)\n",
    "        st.write(\"File Type = \" + chain_mode)\n",
    "        embeddings = load_embeddings(model_name)\n",
    "        hardCoded_path = 'sample_financial_data.csv'\n",
    "        st.write(\"File Name = \" + hardCoded_path)\n",
    "        files = get_csv_file(hardCoded_path)\n",
    "        paths, texts, chroma = None, None, None\n",
    "\n",
    "\n",
    "        if chain_mode == 'Database':\n",
    "            llm_chain, llm = None, None\n",
    "            try:\n",
    "                print(os.environ['DB_ACTIVE'])\n",
    "                if os.environ['DB_ACTIVE'] == \"true\":\n",
    "                    llm_chain, llm = st.session_state['models']\n",
    "                    \n",
    "                else:\n",
    "                    llm_chain, llm = get_db_credentials(model_name=model_name, temperature=temperature,\n",
    "                                                    chain_mode=chain_mode)\n",
    "            except KeyError:\n",
    "                st.sidebar.warning('Provide a Database Log in Details')\n",
    "                os.environ['DB_ACTIVE'] = \"false\"\n",
    "                llm_chain, llm = get_db_credentials(model_name=model_name, temperature=temperature,\n",
    "                                                    chain_mode=chain_mode)\n",
    "                \n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                err = str(e)\n",
    "                st.error(err)\n",
    "                \n",
    "\n",
    "        elif files is not None:\n",
    "            for fp in files:\n",
    "                if fp[0] == 'csv':\n",
    "                    paths = fp[1]\n",
    "                elif fp[0] == 'docs':\n",
    "                    texts = fp[1]\n",
    "            if texts:\n",
    "                import openai\n",
    "                try:\n",
    "                    chroma = build_vector_store(texts, embeddings)\n",
    "                except openai.error.AuthenticationError:\n",
    "                    st.echo('Invalid OPENAI API KEY')\n",
    "            \n",
    "            if chain_mode == \"CSV|Excel\":\n",
    "                if paths is None:\n",
    "                    st.sidebar.warning(\"Note: No CSV or Excel data uploaded. Provide atleast one data source\")\n",
    "                llm_chain, llm = init_agent(model_name, temperature, csv=paths, chain_mode=chain_mode)\n",
    "\n",
    "            elif chain_mode == 'Documents':\n",
    "                try:\n",
    "                    assert chroma != None\n",
    "                    llm_chain, llm = get_retrieval_chain(model_name, temperature, docsearch = chroma)\n",
    "                except AssertionError as e:\n",
    "                    st.sidebar.warning('Upload at least one document')\n",
    "                    llm_chain, llm = None, None\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            if chain_mode == \"CSV|Excel\":\n",
    "                try: \n",
    "                    assert paths != None\n",
    "                except AssertionError as e:\n",
    "                    st.sidebar.warning(\"Note: No CSV data uploaded. Upload at least one csv or excel file\")\n",
    "\n",
    "            elif chain_mode == 'Documents':\n",
    "                try:\n",
    "                    assert chroma != None\n",
    "                except AssertionError as e:\n",
    "                    st.sidebar.warning('Upload at least one document or swith to data query')\n",
    "                    \n",
    "        \n",
    "\n",
    "        init_messages()\n",
    "        # input parameters\n",
    "        \n",
    "\n",
    "        # Supervise user input\n",
    "        st.header(\"Personal FinanceGPT\")\n",
    "        st.header(\"Question\")\n",
    "        user_input = input(\"Input: \")\n",
    "\n",
    "        if user_input:\n",
    "            # Display the entered question\n",
    "            st.write(f\"Entered Question: {user_input}\")\n",
    "\n",
    "            # Get the answer based on the user input\n",
    "            st.header(\"Answer\")\n",
    "            answer = get_answer(llm_chain, llm, user_input)  # Replace 'model' with your language model\n",
    "            st.write(answer[0])\n",
    "        \n",
    "        # if user_input:\n",
    "        #     try:\n",
    "        #         assert type(llm_chain) != type(None)\n",
    "        #         if chroma:\n",
    "        #             context = [c.page_content for c in chroma.similarity_search(\n",
    "        #                 user_input, k=10)]\n",
    "        #             user_input_w_context = PromptTemplate(\n",
    "        #                 template=PROMPT_TEMPLATE,\n",
    "        #                 input_variables=[\"context\", \"question\"]) \\\n",
    "        #                 .format(\n",
    "        #                     context=context, question=user_input)\n",
    "                    \n",
    "        #         else:\n",
    "        #             user_input_w_context = user_input\n",
    "        #         st.session_state.messages.append(\n",
    "        #             HumanMessage(content=user_input_w_context))\n",
    "                \n",
    "                \n",
    "        #         with st.spinner(\"Assistant is typing ...\"):\n",
    "        #             answer, cost = get_answer(llm_chain,llm, user_input)\n",
    "        #             st.write(answer)\n",
    "\n",
    "        #         st.session_state.messages.append(AIMessage(content=answer))\n",
    "        #         st.session_state.costs.append(cost)\n",
    "        #     except AssertionError:\n",
    "        #         st.warning('Please provide a context source')\n",
    "\n",
    "        # Display chat history\n",
    "        messages = st.session_state.get(\"messages\", [])\n",
    "        for message in messages:\n",
    "            if isinstance(message, AIMessage):\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    st.markdown(message.content)\n",
    "            elif isinstance(message, HumanMessage):\n",
    "                with st.chat_message(\"user\"):\n",
    "                    st.markdown(extract_userquesion_part_only(message.content))\n",
    "\n",
    "        costs = st.session_state.get(\"costs\", [])\n",
    "        st.sidebar.markdown(\"## Costs\")\n",
    "        st.sidebar.markdown(f\"**Total cost: ${sum(costs):.5f}**\")\n",
    "        for cost in costs:\n",
    "            st.sidebar.markdown(f\"- ${cost:.5f}\")\n",
    "    except openai.error.AuthenticationError as e:\n",
    "        st.warning(\"Incorrect API key provided: You can find your API key at https://platform.openai.com/account/api-keys\")\n",
    "    except openai.error.RateLimitError:\n",
    "        st.warning('OpenAI RateLimit: Your API Key has probably exceeded the maximum requests per min or per day')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 16:38:11.228 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/app-root/lib64/python3.9/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Finance Chatbot"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OpenAPI Key = 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"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Model = gpt-4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Temperature = 0.5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "File Type = CSV|Excel"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "File Name = sample_financial_data.csv"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Personal FinanceGPT"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Question"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input:  What is this dataset about?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Entered Question: What is this dataset about?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Answer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# streamlit run app.py\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
