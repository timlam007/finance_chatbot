{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from streamlit_jupyter import StreamlitPatcher\n",
    "StreamlitPatcher().jupyter()  # register streamlit with jupyter-compatible wrappers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.SQLagent import build_sql_agent\n",
    "from agents.csv_chat import build_csv_agent\n",
    "from utils.utility import ExcelLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import (SystemMessage, HumanMessage, AIMessage)\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Qdrant\n",
    "import streamlit as st\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.session_state.csv_file_paths = []\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Use the following pieces of context enclosed by triple backquotes to answer the question at the end.\n",
    "\\n\\n\n",
    "Context:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\\n\\n\n",
    "Question: [][][][]{question}[][][][]\n",
    "\\n\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_ai_key():\n",
    "    with st.sidebar:\n",
    "        openai_api_key = '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'\n",
    "        \"[Get an OpenAI API key](https://platform.openai.com/account/api-keys)\"\n",
    "        if not openai_api_key:\n",
    "            st.info(\"Please add your OpenAI API key to continue.\")\n",
    "            st.stop()\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_data\n",
    "def dbActive():\n",
    "    os.environ['DB_ACTIVE'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_page() -> None:\n",
    "    st.set_page_config(\n",
    "    )\n",
    "    st.sidebar.title(\"Options\")\n",
    "    icon, title = st.columns([3, 20])\n",
    "    with icon:\n",
    "        st.image('./img/image.png')\n",
    "    with title:\n",
    "        st.title('Finance Chatbot')\n",
    "    st.session_state['db_active'] = False\n",
    "def init_messages() -> None:\n",
    "    clear_button = st.sidebar.button(\"Clear Conversation\", key=\"clear\")\n",
    "    if clear_button or \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"You are a helpful AI QA assistant. \"\n",
    "                    \"When answering questions, use the context provided to you.\"\n",
    "                    \"If you don't know the answer, just say that you don't know, \"\n",
    "                    \"don't try to make up an answer. \"\n",
    "                    )\n",
    "            )\n",
    "        ]\n",
    "        st.session_state.costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_file(file_path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Function to load PDF text and split it into chunks.\n",
    "    \"\"\"\n",
    "    # Replace st.header and st.file_uploader with the file_path parameter\n",
    "\n",
    "    # Example:\n",
    "    # st.header(\"Upload Document or Connect to a Databse\")\n",
    "    # file_path = \"/path/to/your/file.pdf\"\n",
    "\n",
    "    all_docs = []\n",
    "    csv_paths = []\n",
    "    all_files = []\n",
    "\n",
    "    Loader = None\n",
    "\n",
    "    if file_path.endswith(\".txt\"):\n",
    "        Loader = TextLoader\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        Loader = PyPDFLoader\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        Loader = Docx2txtLoader\n",
    "    elif file_path.endswith(\".csv\"):\n",
    "        # Assuming that the hardcoded path is for a CSV file\n",
    "        csv_paths.append(file_path)\n",
    "    elif file_path.endswith(\".xlsx\"):\n",
    "        # Assuming that the hardcoded path is for an Excel file\n",
    "        loader = ExcelLoader(file_path)\n",
    "        paths = loader.load()\n",
    "        csv_paths.extend(paths)\n",
    "    else:\n",
    "        raise ValueError('File type is not supported')\n",
    "\n",
    "    if Loader:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tpfile:\n",
    "            tpfile.write(file_path)\n",
    "            loader = Loader(tpfile.name)\n",
    "            docs = loader.load()\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "    if all_docs:\n",
    "        documents = text_splitter.split_documents(all_docs)\n",
    "        all_files.append(('docs', documents))\n",
    "    if csv_paths:\n",
    "        all_files.append(('csv', csv_paths))\n",
    "    all_files = tuple(all_files)\n",
    "\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_credentials(model_name, temperature, chain_mode='Database'):\n",
    "    \"\"\"\n",
    "    creates a form for the user to input database login credentials\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the form has already been submitted\n",
    "    \n",
    "    db_active = os.environ['DB_ACTIVE']\n",
    "    if db_active == \"true\":\n",
    "        print(db_active)\n",
    "\n",
    "        return st.session_state['models']\n",
    "        \n",
    "    else:\n",
    "        username = None\n",
    "        host = None\n",
    "        port = None\n",
    "        db = None\n",
    "        password = None\n",
    "        import time\n",
    "        pholder = st.empty()\n",
    "        \n",
    "        with pholder.form('Database_Login'):\n",
    "            st.write(\"Enter Database Credentials \")\n",
    "            username = st.text_input('Username').strip()\n",
    "            password = st.text_input('Password', type='password',).strip()\n",
    "            rdbs = st.selectbox('Select RDBS:',\n",
    "                                (\"Postgres\",\n",
    "                                'MS SQL Server/Azure SQL',\n",
    "                                \"MySQL\",\n",
    "                                \"Oracle\")\n",
    "                            )\n",
    "            port = st.number_input('Port')\n",
    "            host = st.text_input('Hostname').strip()\n",
    "            db = st.text_input('Database name').strip()\n",
    "\n",
    "            submitted = st.form_submit_button('Submit')\n",
    "\n",
    "        if submitted:\n",
    "            with st.spinner(\"Logging into database...\"):\n",
    "                \n",
    "                llm_chain, llm = init_agent(model_name=model_name,\n",
    "                                    temperature=temperature,\n",
    "                                    rdbs = rdbs,\n",
    "                                    username=username,\n",
    "                                    password=password,\n",
    "                                    port=port,\n",
    "                                    host=host,\n",
    "                                    database=db,\n",
    "                                    chain_mode = chain_mode)\n",
    "            st.session_state['models'] = (llm_chain, llm)\n",
    "            st.success(\"Login Success\")\n",
    "            os.environ['DB_ACTIVE'] = \"true\"\n",
    "            db_active = os.environ['DB_ACTIVE']\n",
    "            st.session_state['db_active'] = True\n",
    "            time.sleep(2)\n",
    "            pholder.empty()\n",
    "\n",
    "            # If the form has already been submitted, return the stored models\n",
    "        if db_active == \"true\":\n",
    "            #return st.session_state['models']\n",
    "            mds =  st.session_state['models']\n",
    "            st.write(\"Reached\")\n",
    "            return mds\n",
    "        else:\n",
    "            st.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vector_store(\n",
    "    docs: str, embeddings: Union[OpenAIEmbeddings, LlamaCppEmbeddings]) \\\n",
    "        -> Optional[Qdrant]:\n",
    "    \"\"\"\n",
    "    Store the embedding vectors of text chunks into vector store (Qdrant).\n",
    "    \"\"\"\n",
    "    \n",
    "    if docs:\n",
    "        with st.spinner(\"Loading FIle ...\"):\n",
    "            chroma = Chroma.from_documents(\n",
    "             docs, embeddings\n",
    "            )\n",
    "    \n",
    "        st.success(\"File Loaded Successfully!!\")\n",
    "    else:\n",
    "        chroma = None\n",
    "    return chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model \n",
    "\n",
    "def select_llm() -> Union[ChatOpenAI, LlamaCpp]:\n",
    "    \"\"\"\n",
    "    Read user selection of parameters in Streamlit sidebar.\n",
    "    \"\"\"\n",
    "    model_name = \"gpt-4\"\n",
    "    temperature = \"0.5\"\n",
    "    chain_mode = \"CSV|Excel\"\n",
    "    #api_key  = st.sidebar.text_input('OPENAI API Key')\n",
    "    \n",
    "    return model_name, temperature, chain_mode,# api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_agent(model_name: str, temperature: float, **kwargs) -> Union[ChatOpenAI, LlamaCpp]:\n",
    "    \"\"\"\n",
    "    Load LLM.\n",
    "    \"\"\"\n",
    "    llm_agent = None  # Initialize llm_agent with a default value\n",
    "    \n",
    "    if model_name.startswith(\"gpt-\"):\n",
    "        llm =  ChatOpenAI(temperature=temperature, model_name=model_name)\n",
    "    \n",
    "    elif model_name.startswith(\"text-dav\"):\n",
    "        llm =  OpenAI(temperature=temperature, model_name=model_name)\n",
    "    \n",
    "    elif model_name.startswith(\"llama-2-\"):\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        llm = LlamaCpp(\n",
    "            model_path=f\"./models/{model_name}.bin\",\n",
    "            input={\"temperature\": temperature,\n",
    "                   \"max_length\": 2048,\n",
    "                   \"top_p\": 1\n",
    "                   },\n",
    "            n_ctx=2048,\n",
    "            callback_manager=callback_manager,\n",
    "            verbose=False,  # True\n",
    "        )\n",
    "    chain_mode = kwargs['chain_mode']\n",
    "    if chain_mode == 'Database':\n",
    "        rdbs = kwargs['rdbs']\n",
    "        username = kwargs['username']\n",
    "        password = kwargs['password']\n",
    "        host = kwargs['host']\n",
    "        port = kwargs['port']\n",
    "        database = kwargs['database']\n",
    "        #print('----------------------------------------------------------------')\n",
    "        #st.write(print(rdbs,username,password,host,port,database ))\n",
    "        #print(rdbs,username,password,host,port,database )\n",
    "        llm_agent = build_sql_agent(llm=llm, rdbs=rdbs, username=username, password=password,\n",
    "                                    host=host, port=port, database=database)\n",
    "    if chain_mode == 'CSV|Excel':\n",
    "        file_paths = kwargs['csv']\n",
    "        if file_paths is not None:\n",
    "            with st.spinner(\"Loading CSV FIle ...\"):\n",
    "                llm_agent = build_csv_agent(llm, file_path=file_paths)\n",
    "    \n",
    "    return llm_agent, llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieval_chain(model_name: str, temperature: float, **kwargs) -> Union[ChatOpenAI, LlamaCpp]:\n",
    "    if model_name.startswith(\"gpt-\"):\n",
    "        llm =  ChatOpenAI(temperature=temperature, model_name=model_name)\n",
    "    \n",
    "    elif model_name.startswith(\"text-dav\"):\n",
    "        llm =  OpenAI(temperature=temperature, model_name=model_name)\n",
    "    \n",
    "    elif model_name.startswith(\"llama-2-\"):\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        llm = LlamaCpp(\n",
    "            model_path=f\"./models/{model_name}.bin\",\n",
    "            input={\"temperature\": temperature,\n",
    "                   \"max_length\": 2048,\n",
    "                   \"top_p\": 1\n",
    "                   },\n",
    "            n_ctx=2048,\n",
    "            callback_manager=callback_manager,\n",
    "            verbose=False,  # True\n",
    "        )\n",
    "    docsearch = kwargs['docsearch']\n",
    "    retrieval_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "            llm,\n",
    "            retriever = docsearch.as_retriever(max_tokens_limit=4097)\n",
    "            )\n",
    "        \n",
    "    return retrieval_chain, llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model_name: str) -> Union[OpenAIEmbeddings, LlamaCppEmbeddings]:\n",
    "    \"\"\"\n",
    "    Load embedding model.\n",
    "    \"\"\"\n",
    "    if model_name.startswith(\"gpt-\") or model_name.startswith(\"text-dav\"):\n",
    "        return OpenAIEmbeddings()\n",
    "    elif model_name.startswith(\"llama-2-\"):\n",
    "        return LlamaCppEmbeddings(model_path=f\"./models/{model_name}.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(llm_chain,llm, message) -> tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Get the AI answer to user questions.\n",
    "    \"\"\"\n",
    "    import langchain\n",
    "\n",
    "    if isinstance(llm, (ChatOpenAI, OpenAI)):\n",
    "        with get_openai_callback() as cb:\n",
    "            try:\n",
    "                if isinstance(llm_chain, RetrievalQAWithSourcesChain):\n",
    "                    response = llm_chain(message)\n",
    "                    answer =  str(response['answer'])# + \"\\n\\nSOURCES: \" + str(response['sources'])\n",
    "                else:\n",
    "                    answer = llm_chain.run(message)\n",
    "            except langchain.schema.output_parser.OutputParserException as e:\n",
    "                response = str(e)\n",
    "                if not response.startswith(\"Could not parse tool input: \"):\n",
    "                    raise e\n",
    "                answer = response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
    "        return answer, cb.total_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_role(message: Union[SystemMessage, HumanMessage, AIMessage]) -> str:\n",
    "    \"\"\"\n",
    "    Identify role name from langchain.schema object.\n",
    "    \"\"\"\n",
    "    if isinstance(message, SystemMessage):\n",
    "        return \"system\"\n",
    "    if isinstance(message, HumanMessage):\n",
    "        return \"user\"\n",
    "    if isinstance(message, AIMessage):\n",
    "        return \"assistant\"\n",
    "    raise TypeError(\"Unknown message type.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_langchainschema_to_dict(\n",
    "        messages: List[Union[SystemMessage, HumanMessage, AIMessage]]) \\\n",
    "        -> List[dict]:\n",
    "    \"\"\"\n",
    "    Convert the chain of chat messages in list of langchain.schema format to\n",
    "    list of dictionary format.\n",
    "    \"\"\"\n",
    "    return [{\"role\": find_role(message),\n",
    "             \"content\": message.content\n",
    "             } for message in messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_userquesion_part_only(content):\n",
    "    \"\"\"\n",
    "    Function to extract only the user question part from the entire question\n",
    "    content combining user question and pdf context.\n",
    "    \"\"\"\n",
    "    content_split = content.split(\"[][][][]\")\n",
    "    if len(content_split) == 3:\n",
    "        return content_split[1]\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    import openai\n",
    "    init_page()\n",
    "    dbActive()\n",
    "    try:\n",
    "        open_ai_key()\n",
    "        if 'history' not in st.session_state:\n",
    "            st.session_state['history'] = []\n",
    "        \n",
    "        model_name, temperature, chain_mode = select_llm()\n",
    "        embeddings = load_embeddings(model_name)\n",
    "        files = get_csv_file('sample_financial_data.csv')\n",
    "        paths, texts, chroma = None, None, None\n",
    "\n",
    "        if chain_mode == 'Database':\n",
    "            llm_chain, llm = None, None\n",
    "            try:\n",
    "                print(os.environ['DB_ACTIVE'])\n",
    "                if os.environ['DB_ACTIVE'] == \"true\":\n",
    "                    llm_chain, llm = st.session_state['models']\n",
    "                    \n",
    "                else:\n",
    "                    llm_chain, llm = get_db_credentials(model_name=model_name, temperature=temperature,\n",
    "                                                    chain_mode=chain_mode)\n",
    "            except KeyError:\n",
    "                st.sidebar.warning('Provide a Database Log in Details')\n",
    "                os.environ['DB_ACTIVE'] = \"false\"\n",
    "                llm_chain, llm = get_db_credentials(model_name=model_name, temperature=temperature,\n",
    "                                                    chain_mode=chain_mode)\n",
    "                \n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                err = str(e)\n",
    "                st.error(err)\n",
    "                \n",
    "\n",
    "        elif files is not None:\n",
    "            for fp in files:\n",
    "                if fp[0] == 'csv':\n",
    "                    paths = fp[1]\n",
    "                elif fp[0] == 'docs':\n",
    "                    texts = fp[1]\n",
    "            if texts:\n",
    "                import openai\n",
    "                try:\n",
    "                    chroma = build_vector_store(texts, embeddings)\n",
    "                except openai.error.AuthenticationError:\n",
    "                    st.echo('Invalid OPENAI API KEY')\n",
    "            \n",
    "            if chain_mode == \"CSV|Excel\":\n",
    "                if paths is None:\n",
    "                    st.sidebar.warning(\"Note: No CSV or Excel data uploaded. Provide atleast one data source\")\n",
    "                llm_chain, llm = init_agent(model_name, temperature, csv=paths, chain_mode=chain_mode)\n",
    "\n",
    "            elif chain_mode == 'Documents':\n",
    "                try:\n",
    "                    assert chroma != None\n",
    "                    llm_chain, llm = get_retrieval_chain(model_name, temperature, docsearch = chroma)\n",
    "                except AssertionError as e:\n",
    "                    st.sidebar.warning('Upload at least one document')\n",
    "                    llm_chain, llm = None, None\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            if chain_mode == \"CSV|Excel\":\n",
    "                try: \n",
    "                    assert paths != None\n",
    "                except AssertionError as e:\n",
    "                    st.sidebar.warning(\"Note: No CSV data uploaded. Upload at least one csv or excel file\")\n",
    "\n",
    "            elif chain_mode == 'Documents':\n",
    "                try:\n",
    "                    assert chroma != None\n",
    "                except AssertionError as e:\n",
    "                    st.sidebar.warning('Upload at least one document or swith to data query')\n",
    "                    \n",
    "        \n",
    "\n",
    "        init_messages()\n",
    "\n",
    "        # Supervise user input\n",
    "        st.header(\"Personal FinanceGPT\")\n",
    "        st.header(\"Question\")\n",
    "        user_input = \"Based on the financial data from the sample, which industry seems to have the best ratio of revenue to expenses, and what does that suggest about their operational efficiency? Display the ratios for all industries in decending order in a table format\"\n",
    "        st.write(user_input)\n",
    "        st.header(\"Answer\")\n",
    "        answer = get_answer(llm_chain,llm, user_input)\n",
    "        st.write(answer[0])\n",
    "            \n",
    "        # if user_input:\n",
    "        #     try:\n",
    "        #         assert type(llm_chain) != type(None)\n",
    "        #         if chroma:\n",
    "        #             context = [c.page_content for c in chroma.similarity_search(\n",
    "        #                 user_input, k=10)]\n",
    "        #             user_input_w_context = PromptTemplate(\n",
    "        #                 template=PROMPT_TEMPLATE,\n",
    "        #                 input_variables=[\"context\", \"question\"]) \\\n",
    "        #                 .format(\n",
    "        #                     context=context, question=user_input)\n",
    "                    \n",
    "        #         else:\n",
    "        #             user_input_w_context = user_input\n",
    "        #         st.session_state.messages.append(\n",
    "        #             HumanMessage(content=user_input_w_context))\n",
    "                \n",
    "                \n",
    "        #         with st.spinner(\"Assistant is typing ...\"):\n",
    "        #             answer, cost = get_answer(llm_chain,llm, user_input)\n",
    "        #             st.write(answer)\n",
    "\n",
    "        #         st.session_state.messages.append(AIMessage(content=answer))\n",
    "        #         st.session_state.costs.append(cost)\n",
    "        #     except AssertionError:\n",
    "        #         st.warning('Please provide a context source')\n",
    "\n",
    "        # Display chat history\n",
    "        messages = st.session_state.get(\"messages\", [])\n",
    "        for message in messages:\n",
    "            if isinstance(message, AIMessage):\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    st.markdown(message.content)\n",
    "            elif isinstance(message, HumanMessage):\n",
    "                with st.chat_message(\"user\"):\n",
    "                    st.markdown(extract_userquesion_part_only(message.content))\n",
    "\n",
    "        costs = st.session_state.get(\"costs\", [])\n",
    "        st.sidebar.markdown(\"## Costs\")\n",
    "        st.sidebar.markdown(f\"**Total cost: ${sum(costs):.5f}**\")\n",
    "        for cost in costs:\n",
    "            st.sidebar.markdown(f\"- ${cost:.5f}\")\n",
    "    except openai.error.AuthenticationError as e:\n",
    "        st.warning(\"Incorrect API key provided: You can find your API key at https://platform.openai.com/account/api-keys\")\n",
    "    except openai.error.RateLimitError:\n",
    "        st.warning('OpenAI RateLimit: Your API Key has probably exceeded the maximum requests per min or per day')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Finance Chatbot"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Personal FinanceGPT"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Question"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Based on the financial data from the sample, which industry seems to have the best ratio of revenue to expenses, and what does that suggest about their operational efficiency? Display the ratios for all industries in decending order in a table format"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Answer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: To answer this question, I need to calculate the ratio of revenue to expenses for each industry. This can be achieved by creating a new column in the dataframe which is the result of dividing the 'Revenue (millions)' column by the 'Expenses (millions)' column. After that, I can sort the dataframe by this new column in descending order and display the result. This ratio will give us an idea about the operational efficiency of each industry, the higher the ratio the more efficient the industry is in generating revenue relative to its expenses.\n",
      "Action: python_repl_ast\n",
      "Action Input: df['Revenue/Expenses Ratio'] = df['Revenue (millions)'] / df['Expenses (millions)']\n",
      "df_sorted = df.sort_values(by='Revenue/Expenses Ratio', ascending=False)\n",
      "df_sorted[['Industry', 'Revenue/Expenses Ratio']]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m        Industry  Revenue/Expenses Ratio\n",
      "0    Agriculture                2.984382\n",
      "1  Manufacturing                2.888165\n",
      "4     Healthcare                1.602952\n",
      "2     Technology                1.194698\n",
      "3         Retail                1.075562\n",
      "5        Finance                0.667711\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have successfully calculated the ratio of revenue to expenses for each industry and sorted them in descending order. The industry with the highest ratio is Agriculture, followed by Manufacturing and Healthcare. This suggests that Agriculture is the most operationally efficient industry in terms of generating revenue relative to its expenses. The Retail and Finance industries appear to be less efficient in this regard.\n",
      "Final Answer: The industry with the best ratio of revenue to expenses is Agriculture, which suggests that it is the most operationally efficient in terms of generating revenue relative to its expenses. Here are the ratios for all industries in descending order:\n",
      "\n",
      "| Industry       | Revenue/Expenses Ratio |\n",
      "|----------------|------------------------|\n",
      "| Agriculture    | 2.984382               |\n",
      "| Manufacturing  | 2.888165               |\n",
      "| Healthcare     | 1.602952               |\n",
      "| Technology     | 1.194698               |\n",
      "| Retail         | 1.075562               |\n",
      "| Finance        | 0.667711               |\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The industry with the best ratio of revenue to expenses is Agriculture, which suggests that it is the most operationally efficient in terms of generating revenue relative to its expenses. Here are the ratios for all industries in descending order:\n",
       "\n",
       "| Industry       | Revenue/Expenses Ratio |\n",
       "|----------------|------------------------|\n",
       "| Agriculture    | 2.984382               |\n",
       "| Manufacturing  | 2.888165               |\n",
       "| Healthcare     | 1.602952               |\n",
       "| Technology     | 1.194698               |\n",
       "| Retail         | 1.075562               |\n",
       "| Finance        | 0.667711               |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# streamlit run app.py\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance_chatbot",
   "language": "python",
   "name": "finance_chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
